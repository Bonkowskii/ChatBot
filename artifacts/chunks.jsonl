{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 0, "text": "# Modele GPT – szczegółowy przegląd dla RAG (stan na sierpień 2025) Generative Pre‑trained Transformer (GPT) to seria modeli językowych firmy **OpenAI**."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 1, "text": "to seria modeli językowych firmy **OpenAI**. Każda kolejna generacja zwiększa liczbę parametrów, długość okna kontekstowego i zakres zastosowań. Poniższy plik zawiera usystematyzowaną wiedzę na temat modeli GPT od wersji 1 do najnowszych odmian 2025 r., uzupełnioną o informacje o wymaganiach pamięci i licencjach. Modele GPT‑3/3."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 2, "text": "Modele GPT‑3/3. 5/4 dostępne są tylko poprzez API; jedynymi otwartymi wagami od OpenAI są nowo zaprezentowane modele **gpt‑oss**. ## 1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 3, "text": "1. Ewolucja serii GPT\n\n| Generacja | Liczba parametrów | Okno kontekstowe i wykorzystanie | Wymagania pamięci\na (przybliżone do ład. inference) | Licencja i dostępność | Cechy i zastosowania |\n|---|---|---|---|---|---|\n| **GPT‑1** (czerwiec 2018) | ~117 mln parametrówhttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models | model bazowy używający 12‑warstwowego dekodera transformera; kontekst ok. 512 tokenów (nieformalnie podawane w artykułach technicznych); trenowany na korpusie **BooksCorpus**; generował płynny tekst, ale radził sobie tylko z krótkimi sekwencjamihttps://www.makeuseof.com/gpt-models-explained-and-compared/#:~:text=GPT,art%20language%20models. | Wagi mają ~0,5 GB; do uruchomienia w FP16 wystarcza konsumencki GPU. | Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 4, "text": "| Wydany na licencji MIT; wagi udostępniono publicznie poprzez GitHub. | Pokazał siłę pre‑trenowania na dużym zbiorze i zapoczątkował serię GPT. |\n| **GPT‑2** (luty 2019) | 1,5 mld parametrów (wersja pełna) ; udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 5, "text": "; udostępniono także warianty **124 M**, **355 M** i **774 M**https://en.wikipedia.org/wiki/GPT-2#:~:text =On%20August%2020%2C%202019%2C%20OpenAI,24. | Używa kontekstu 1 024 tokenów (dwukrotnie dłuższego niż GPT‑1). Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 6, "text": "Pełen model generował płynny tekst w wielu stylach, ale „rozsypywał się” w długich akapitachhttps://en.wikipedia.org/wiki/GPT-2#:~:text=%5BImage%202%5DGPT,sentence%20repeated%20over%20and%20over. | Wagi modelu pełnego mają ok. 5 GB i pochłaniają dużo RAM podczas inferencji; nawet pojedyncze zapytanie potrafiło obciążyć CPU w 100%https://en.wikipedia.org/wiki/GPT-2#:~:text=GPT,To%20alleviate%20these%20issues. Dla FP32 potrzeba ~6 GB VRAM, co potwierdzają przewodniki dotyczące uruchamiania GPT‑2https://shayangeek.com/en/estimate-gpu-memory-needed-for-ai-model/#:~:text =models%20with%20a%20larger%20number,precision."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 7, "text": "=models%20with%20a%20larger%20number,precision. | Licencja MIT; OpenAI w pierwszej kolejności wstrzymał publikację pełnego modelu, udostępniając mniejsze wersje. | Model potrafił generować dłuższy, spójny tekst i odpowiadać na proste zapytania; zastosowania: kreatywne pisanie, tłumaczenia i generowanie kodu. |\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 8, "text": "|\n| **GPT‑3** (czerwiec 2020) | 175 mld parametrówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=model%20with%20over%20100%20times,the%20chatbot%20product%20%20102. | Standardowe okno kontekstowe ma 2 048 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; model potrafi wykonywać „few‑shot learning” i obsługiwać zadania bez fine‑tuningu. Składa się w 60 % z danych Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 9, "text": "Common Crawl, 22 % z książek i treści licencjonowanych oraz 3 % z Wikipediihttps://en.wikipedia.org/wiki/GPT-3#:~:text=predicts%20to%20be%20most%20relevant.,2. | Dla fp16/bfloat16 potrzeba ~350 GB pamięci GPU (2 bajty × 175 mld parametrów)https://en.wikipedia.org/wiki/GPT-3#:~:text =predicts%20to%20be%20most%20relevant.,2; w praktyce model jest dostępny tylko w chmurze. | Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 10, "text": "| Model komercyjny – dostępny wyłącznie poprzez API OpenAI/Microsoft; prawa do wag posiada Microsoft. | Umożliwił generowanie esejów, kodu, dialogów i tłumaczeń; stał się podstawą wielu aplikacji oraz powstania ChatGPT. |\n| **GPT‑3. 5 / ChatGPT** (2022)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 11, "text": "5 / ChatGPT** (2022) | ok. 175 mld parametrów, model „InstructGPT” fine‑tuningowany z użyciem RLHF. | Pierwsza wersja ChatGPT miała okno 4 096 tokenów; w przeszłość 2024 r. wprowadzono **GPT‑3. 5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 12, "text": "5 Turbo** z oknem 16 385 tokenów oraz limitem odpowiedzi 4 096 tokenówhttps://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=What%20is%20the%20ChatGPT,Limit. | VRAM jest podobny jak dla GPT‑3 (kilkaset GB w pełnej precyzji), natomiast podczas inferencji w chmurze obliczenia są rozdzielane na wiele GPU. | Model własnościowy; udostępniany poprzez usługę **ChatGPT** (free/Plus) i API, licencja zastrzeżona (regulamin zabrania użycia do generowania innych modeli). | ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 13, "text": "| ChatGPT wprowadził przyjazny interfejs rozmowy; uzyskał szerokie zastosowanie w asystentach, generowaniu treści, analizie danych, programowaniu i rozmowach. |\n| **GPT‑4** (marzec 2023) | Dokładna liczba parametrów nieujawniona; szacunkowo ~1 bilion (niektóre źródła wskazują ~1,76 biliona)https://tactiq.io/learn/token-limit-chatgpt-3-5-and-chatgpt-4#:~:text=GPT,of%20data%20until%20April%202023. | Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 14, "text": "| Wersje mają okna 8 192 i 32 768 tokenówhttps://en.wikipedia.org/wiki/GPT-4#:~:text=of%20GPT,4%20were%20predicted%20by%20OpenAI; w listopadzie 2023 wprowadzono **GPT‑4 Turbo**, które oferuje okno **128 k** i niższą cenęhttps://en.wikipedia.org/wiki/GPT-4#:~:text=In%20November%202023%2C%20OpenAI%20announced,23. | Ze względu na ogromną liczbę parametrów model wymaga setek gigabajtów VRAM; szczegóły sprzętowe nie zostały upublicznione. | Licencja komercyjna; dostępny wyłącznie w API i płatnych subskrypcjach ChatGPT Plus/Enterprise. | Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 15, "text": "| Model multimodalny – przyjmuje tekst i obrazy oraz potrafi opisywać zawartość zdjęć i rozwiązywać zadania graficznehttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. Przewyższa GPT‑3. 5 w rozumieniu poleceń i kodowaniu. |\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 16, "text": "|\n| **GPT‑4o (\"Omni\")** (maj 2024) | Stanowi rozszerzenie GPT‑4 z niewielką liczbą parametrów (szczegóły niejawne). | Zachowuje 128 k kontekstu, ale dodaje **multimodalny input/ output**: model generuje i analizuje tekst, dźwięk oraz obraz w czasie rzeczywistymhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. | Wymaga mniejszej mocy niż pełne GPT‑4, jednak szczegóły pamięci są tajne; działa poprzez API. | Komercyjny, dostępny w ChatGPT Plus i wybranych API."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 17, "text": "| Komercyjny, dostępny w ChatGPT Plus i wybranych API. | Pozwala na konwersacje audio w realnym czasie, rozumie mowę i języki nieangielskie oraz osiąga rekordowe wyniki w transkrypcji i tłumaczeniachhttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. |\n| **GPT‑4.5** (wczesny 2025) | Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów;"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 18, "text": "| Według informacji prasowych to ogromny model o bardzo dużej liczbie parametrów; OpenAI opisuje go jako „gigantyczny, kosztowny model”https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,%E2%80%9Cgiant%2C%20expensive%20model%2C%E2%80%9D%20with%20API. | Ma podobne lub większe okno kontekstowe niż GPT‑4 Turbo (≥128 k). | Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 19, "text": "| Wyjątkowo zasobożerny; wykorzystywany głównie przez partnerów korporacyjnych poprzez API. | Komercyjny – model „ogólnej wiedzy”; niedostępny jako wagi open‑source. | Ulepszona wersja GPT‑4 z większą bazą wiedzy i większym „EQ”; generuje bardziej naturalne i pogłębione odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 20, "text": "odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=the%20O,shortly%2C%20along%20with%20its%20specialized. |\n| **GPT‑4. 1** (kwiecień 2025) | Ulepszona wersja GPT‑4o skoncentrowana na kodowaniu i długich kontekstachhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,context%20window%2C%20vastly%20exceeding%20the. | **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 21, "text": "| **Obsługuje kontekst do 1 mln tokenów** – dziesięciokrotnie więcej niż GPT‑4 Turbohttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. Wprowadza sub‑modele mini i nano oraz obsługę „tools” w API. | Ze względu na rekordowe okno wymaga zaawansowanego sprzętu; detale są tajne. | Dostępny tylko poprzez API; licencja komercyjna."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 22, "text": "| Dostępny tylko poprzez API; licencja komercyjna. | Zaprojektowany do generowania kodu, precyzyjnego podążania za instrukcjami i analizy bardzo długich dokumentówhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=,an%20order%20of%20magnitude%20jump. |\n| **O‑series (O3, O4‑mini, O4 etc.)** (2024‑2025) | Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 23, "text": "| Modele dedykowane do *reasoningu* i agentowego korzystania z narzędzi – O3 (2024) i O4‑mini (kwiecień 2025) są mniejsze niż standardowe GPT‑4. | O‑seria wykonuje wewnętrzną „chain‑of‑thought”, pozwalając modelowi na planowanie i wykonywanie kroków, a następnie udostępnienie finalnej odpowiedzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. O4‑mini osiąga znakomite wyniki w testach matematycznych i programistycznychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. | Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 24, "text": "| Modele te wymagają mniejszych zasobów niż pełne GPT‑4; O4‑mini ma czas reakcji zbliżony do ChatGPT, lecz nadal potrzebuje GPU. | Komercyjne; dostępne w ChatGPT i wybranych API. | O‑seria potrafi korzystać z narzędzi (wyszukiwarki, interpreter Python), co umożliwia rozwiązywanie zadań matematycznych, analitycznych i kodowychhttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. |\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 25, "text": "|\n| **GPT‑OSS** (sierpień 2025) – **open‑weights** | **gpt‑oss‑120b**: 117 mld parametrów, 5,1 mld aktywnych parametrów; **gpt‑oss‑20b**: 21 mld parametrów, 3,6 mld aktywnychfile:///home/oai/redirect.html#:~:text =GPT%20OSS%20is%20a%20hugely,perfect%20for%20consumer%20hardware%20andfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Modele korzystają z kwantyzacji **MXFP4** i obsługują 128 k tokenów dzięki pozycjonowaniu RoPE; warstwy uwagi naprzemiennie działają w trybie pełnego kontekstu i przesuwającego okna 128‑tokenowegofile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy.  **gpt‑oss‑120b** zmieści się na jednej karcie **H100 80 GB**, a **gpt‑oss‑20b** można uruchomić na GPU z 16 GB VRAMfile:///home/oai/redirect.html#:~:text =GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 26, "text": "=GPT%20OSS%20is%20a%20hugely,device%20applicationsfile:///home/oai/redirect.html#:~:text=,in%20a%20single%2016GB%20GPU. | Pliki wag wymagają 80 GB (120b) lub 16 GB (20b) pamięci VRAM dzięki 4‑bitowej kwantyzacjifile:///home/oai/redirect.html#:~:text =GPT%20OSS%20is%20a%20hugely,device%20applications. | **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 27, "text": "| **Licencja Apache 2.0** z niewielkim uzupełnieniem dotyczącym bezpiecznego użyciafile:///home/oai/redirect.html#:~:text =To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy. | Modele tekstowe ukierunkowane na rozumowanie; obsługują chain‑of‑thought, regulowaną „siłę myślenia”, instrukcje i narzędziafile:///home/oai/redirect.html#:~:text=,a%20small%20complementary%20use%20policy. Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 28, "text": "Dzięki otwartym wagom umożliwiają lokalny RAG, fine‑tuning oraz uruchamianie w narzędziach takich jak transformers, vLLM czy llama.cpp. |\n\n## 2. Kluczowe wnioski i zalecenia dla RAG\n\n1. **Parametry i kontekst** – w serii GPT liczba parametrów wzrosła z 117 mln (GPT‑1) do ok. 1 biliona (GPT‑4) , a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 29, "text": ", a kontekst z 512 do nawet **1 mln tokenów** (GPT‑4.1). Dłuższe okna kontekstowe są szczególnie przydatne w systemach RAG, ponieważ umożliwiają modelowi analizę długich dokumentów lub wielu fragmentów na raz.\n\n2. **Wymagania sprzętowe** – komercyjne modele GPT‑3/4 są dostępne wyłącznie jako usługi chmurowe i wymagają setek gigabajtów pamięci GPU do załadowania wag. Wersja **gpt‑oss‑120b** mieści się na pojedynczym H100 (80 GB), a **gpt‑oss‑20b** działa nawet na karcie konsumenckiej z 16 GB VRAMfile:///home/oai/redirect.html#:~:text =GPT%20OSS%20is%20a%20hugely,device%20applications."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 30, "text": "=GPT%20OSS%20is%20a%20hugely,device%20applications. Jeżeli planujesz lokalny RAG, rozważ gpt‑oss lub inne otwarte modele z mniejszą liczbą parametrów, ponieważ GPT‑3/4 wymagają klastrów GPU.\n\n3. **Licencje i wykorzystanie** – wczesne modele GPT‑1 i GPT‑2 opublikowano na licencji MIT; jednak od GPT‑3 wszystkie wagi (z wyjątkiem gpt‑oss) pozostają własnością OpenAI i udostępniane są tylko przez API. Oznacza to, że do budowy własnego systemu RAG musisz używać modeli open‑source (np. gpt‑oss, Llama czy Mistral) lub korzystać z API w chmurze. Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 31, "text": "Modele gpt‑oss dostępne są na licencji **Apache 2.0**, co umożliwia komercyjne wykorzystanie i dalsze modyfikacjefile:///home/oai/redirect.html#:~:text =To%20make%20it%20even%20better,with%20a%20minimal%20usage%20policy.\n\n4. **Zastosowania** – GPT‑3 umożliwił generowanie esejów, programów i dialogów, natomiast GPT‑3. 5 wprowadził RLHF i lepszą konwersację (ChatGPT). GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 32, "text": "GPT‑4 rozszerzył możliwości na multimodalność (obrazy), a GPT‑4o dodał wejście i wyjście audio oraz skrócił czas reakcjihttps://en.wikipedia.org/wiki/GPT-4#:~:text=On%20May%2013%2C%202024%2C%20OpenAI,24. GPT‑4. 1 oraz O‑seria w 2025 r. pokazały nowy kierunek – modele potrafią analizować bardzo długie konteksty oraz korzystać z narzędzi (przeglądarka, interpreter Python, generowanie obrazów)https://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=The%20OpenAI%20O4%20Series%3A%20AI,that%20%E2%80%9CThinks%E2%80%9D%20Before%20Answering. Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 33, "text": "Dla lokalnych systemów RAG zaleca się stosowanie gpt‑oss, ponieważ umożliwiają one pełną kontrolę nad modelem i dane mogą pozostać wewnątrz organizacji.\n\n5. **Benchmarki i wyniki** – OpenAI nie ujawnia pełnych benchmarków dla GPT‑4/4.5/4.1, ale zewnętrzne testy pokazują, że model osiąga wysokie wyniki na egzaminach standaryzowanych (np. SAT, LSAT)https://en.wikipedia.org/wiki/GPT-4#:~:text =GPT,28%20%5D%20an. O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 34, "text": "O‑seria (O3, O4‑mini) ustanowiła rekord na konkursie AIME 2025, rozwiązując prawie wszystkie zadania matematyczne z użyciem narzędzihttps://medium.com/@roberto.g.infante/openais-o4-and-gpt-4-1-a-new-chapter-in-ai-language-models-05103b53794c#:~:text=slow.%20That%E2%80%99s%20where%20O4,performing%20model%20on%20that. ## 3. Podsumowanie\n\nSeria GPT ewoluowała od niewielkiego modelu z 2018 r. do ogromnych systemów multimodalnych, które mogą przetwarzać milion tokenów i rozwiązywać złożone zadania. W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\gpt.txt", "chunk_id": 35, "text": "W kontekście RAG kluczowe jest zrozumienie, że modele GPT > 3 są własnościowe i wymagają ogromnych zasobów; dlatego w praktycznych wdrożeniach, zwłaszcza on‑premise, lepiej korzystać z otwartych modeli takich jak **gpt‑oss**, Llama, Mistral lub innych LLM open‑source. Jeśli planujesz użyć GPT‑3/4 w RAG, pamiętaj o limitach tokenów (4k–128k) oraz kosztach API; natomiast gpt‑oss zapewnia 128 k kontekstu i licencję Apache 2.0, co czyni go atrakcyjnym wyborem do długich dokumentów i rozbudowanych pipeline’ów."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 0, "text": "# Rodzina modeli LLaMA – przegląd i wymagania sprzętowe (stan na sierpień 2025 r.)\n\n# # Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 1, "text": "Wprowadzenie\n\n**LLaMA (Large Language Model Meta AI)** to rodzina dużych modeli językowych udostępnianych przez Meta. Pierwsza generacja (LLaMA 1) została wydana w 2023 r. jako eksperymentalny model badawczy, natomiast kolejne generacje stopniowo zwiększały liczbę parametrów, okno kontekstowe i możliwości multimodalne. Meta udostępnia wagi modeli na warunkach licencji **Llama Community License** – umożliwia ona niekomercyjne wykorzystanie i komercyjne wykorzystanie (z wyjątkami dla podmiotów przekraczających 700 mln aktywnych użytkowników miesięcznie), jednak nakłada obowiązek umieszczania wzmianki „Built with Llama” oraz zakazuje wykorzystywania modeli do trenowania konkurencyjnych LLM‑ówhttps://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text =META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 2, "text": "=META%20LLAMA%203%20COMMUNITY%20LICENSE,AGREEMENT. Modele 3.2 i 4 wprowadzają dodatkowe ograniczenia dotyczące przetwarzania obrazu w Unii Europejskiejhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants i są objęte akceptowalną polityką użycia (Acceptable Use Policy). W poniższym zestawieniu zebrano informacje o poszczególnych generacjach LLaMA, w tym dostępne warianty modelu, minimalne wymagania pamięci RAM/VRAM do uruchomienia, licencję, główne zastosowania oraz wybrane wyniki benchmarków. Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.\n\n## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 3, "text": "Dane te są przydatne do projektowania systemów **retrieval‑augmented generation (RAG)** – można na ich podstawie dobrać model do konkretnego zadania, oszacować zasoby sprzętowe oraz uwzględnić ograniczenia licencyjne.\n\n## LLaMA 1 (2023)\n\nLLaMA 1 była pierwszą publicznie dostępną rodziną modeli Mety. Obejmuje warianty **7 B**, **13 B** i **65 B** parametrów. Modele te obsługują kontekst ok. 2 k tokenów i były udostępnione wyłącznie na zasadach badawczych. * **"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 4, "text": "* ** Liczba parametrów i sprzęt** – wariant 7 B w precyzji FP16 wymaga ok. 12–13 GB pamięci VRAM, natomiast wersje z kwantyzacją (np. 8‑bit lub 4‑bit) można uruchomić na kartach z 6 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI. Model 13 B w pełnej precyzji wymaga ~24 GB VRAM, a kwantyzowany – około 10 GBhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI. Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 5, "text": "Największy wariant 65 B potrzebuje ponad 130 GB VRAM i zwykle wymaga wielu kart GPUhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI. * **Licencja i wykorzystanie** – LLaMA 1 była udostępniona w ramach licencji badawczej (Research Use License). Nie zezwalała ona na komercyjne zastosowania i ograniczała modyfikację i dystrybucję. Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 6, "text": "Modele te były wykorzystywane głównie do badań nad fine‑tuningiem, adaptacją do zadań dziedzinowych i porównania z modelami OpenAI. ## LLaMA 2 (lipiec 2023)\n\nLLaMA 2 wprowadziła komercyjnie dostępne modele **7 B**, **13 B** i **70 B**. Modele zostały wytrenowane na dużo większym zbiorze danych i są licencjonowane na zasadach **Llama Community License**. Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 7, "text": "Licencja pozwala na komercyjne wykorzystanie, ale podmioty posiadające więcej niż 700 mln aktywnych użytkowników miesięcznie muszą uzyskać dodatkową zgodę Metyhttps://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text=The%20Llama%202%20license%C2%A0permits%20any,compliant. * ** Wymagania sprzętowe** – serwis Hardware Corner podaje, że 7‑miliardowy model w 4‑bitowej kwantyzacji (GPTQ) wymaga **ok. 6 GB VRAM**, a inference na CPU w formacie GGML/GGUF potrzebuje **4 GB RAM i 300 MB VRAM**https://www.hardware-corner.net/llm-database/Llama-2/#:~:text =Below%20are%20the%20Llama,bit%20quantization."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 8, "text": "=Below%20are%20the%20Llama,bit%20quantization. Wariant 13 B wymaga **10 GB VRAM** lub 8 GB RAM (GGML), natomiast model 70 B potrzebuje co najmniej **40 GB VRAM** i 64 GB RAM przy użyciu CPUhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models.  \n* **Zastosowania** – LLaMA 2 stała się bazą do generowania tekstu, tłumaczeń, asystentów głosowych i systemów RAG. Dzięki licencji otwartych wag powstały liczne adaptacje, m.in. **Code Llama** (modele do generacji kodu) oraz narzędzia do fine‑tuningu na ograniczonych zasobach. ###"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 9, "text": "### Code Llama (sierpień 2023) Code Llama to wariant LLaMA 2 specjalizujący się w generacji kodu i obsłudze zadań programistycznych. Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 10, "text": "Modele są dostępne w rozmiarach 7 B, 13 B oraz 30/33/34 B i działają z długim kontekstem (do 100 k tokenów). * **Sprzęt** – modele 7 B i 13 B w 4‑bitowej kwantyzacji wymagają odpowiednio **~6 GB** i **~10 GB VRAM**, natomiast warianty ~30 B/34 B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 11, "text": "B potrzebują **ok. 20 GB VRAM**https://www.hardware-corner.net/llm-database/CodeLlama/#:~:text =Hardware%20requirements. Pliki w formacie GGML (inferencja na CPU) potrzebują 4 GB RAM dla 7 B i ok. 8 GB RAM dla 13 Bhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM. * **Licencja**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 12, "text": "* **Licencja** – Code Llama dziedziczy licencję Llama Community License (podobne zasady jak LLaMA 2). Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 13, "text": "Modele są wykorzystywane w edytorach kodu, automatyzacji refaktoryzacji, weryfikacji poprawności programów i w agentach programistycznych. ## LLaMA 3 (kwiecień 2024) Trzecia generacja przyniosła znaczące ulepszenia: zwiększone konteksty, wysoki poziom dokładności, odświeżony tokenizer i bardziej zróżnicowany zbiór danych (7 bln tokenów). Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 14, "text": "Dostępne modele to **8 B** i **70 B**.  \n\n* **Architektura** – wprowadzono **Grouped Query Attention (GQA)** oraz nowy tokenizer, co poprawia przepływ kontekstu i obsługę dłuższych sekwencji. * ** Wymagania sprzętowe** – artykuł firmy Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 15, "text": "Picovoice podaje, że model **Llama 3 70 B** w pełnej precyzji potrzebuje **ponad 140 GB VRAM**, a specjalne techniki kwantyzacji (np. 4‑bit) pozwalają uruchomić go na pojedynczym GPU RTX 4090 (24 GB) kosztem wydajnościhttps://picovoice.ai/blog/unleash-the-power-of-llama3-70b-on-your-everyday-computer/#:~:text=family%20ai,uncommon%20and%20can%20be%20expensive. Analiza Parseur dla wariantu **8 B** wskazuje, że do uruchomienia w chmurze potrzeba GPU z co najmniej **16 GB VRAM** oraz 16 GB RAMhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8B, a model zajmuje ok. 15 GB na dyskuhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=%2A%20Meta,are%20too%20old%20and%20no. Raport LiquidMetal AI potwierdza, że 8‑miliardowy model można uruchomić na sprzęcie konsumenckim z 16 GB VRAMhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text=Pros, natomiast model 70 B wymaga co najmniej 48 GB VRAM w konfiguracji wielo‑GPUhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text =Pricing.  \n* **Licencja** – Llama"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 16, "text": "=Pricing.  \n* **Licencja** – Llama 3 dziedziczy Llama Community License z obowiązkiem oznaczania modeli i przestrzegania polityki akceptowalnego użycia.  \n* **Zastosowania** – LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 17, "text": "– LLaMA 3 jest wykorzystywana w asystentach konwersacyjnych, systemach generujących treści i narzędziach RAG. Dzięki wysokiej wydajności w benchmarkach (np. MMLU, BIG‑BENCH Hard) i efektywnej implementacji GQA, model 8 B jest popularny w aplikacjach on‑device. ## LLaMA 3.1 (lipiec 2024) Wersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 18, "text": "Wersja 3.1 wprowadziła modele **8 B**, **70 B** i **405 B**, rozszerzając kontekst do 128 k tokenów i poprawiając stabilność. Model 405 B należy do największych otwartych modeli udostępnionych przez Metę. * **Wymagania VRAM** – oficjalny artykuł z Hugging Face opisuje zapotrzebowanie na pamięć dla różnych precyzji: \n  * **8 B** – w precyzji FP16 wymaga ok. 16 GB VRAM, w FP8 8 GB, a w INT4 4 GBhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. \n  * **70 B** – potrzebuje około **140 GB VRAM** (FP16), **70 GB** (FP8) lub **35 GB** (INT4)https://huggingface.co/blog/llama31#:~:text =Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 19, "text": "=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB.  \n  * **405 B** – wymaga **810 GB VRAM** (FP16), **405 GB** (FP8) lub **203 GB** w INT4https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB. Warto pamiętać, że okno 128 k tokenów powoduje dodatkowy narzut na pamięć – artykuł wskazuje, że przy 128 k tokenów model 70 B potrzebuje dodatkowych ~39 GB VRAM na **cache** kluczy i wartościhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are. * **Zastosowania** – LLaMA 3.1 oferuje lepsze wnioskowanie i funkcje kodowania niż LLaMA 3, ale kosztem dużo większych wymagań pamięciowych. Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.\n\n## LLaMA 3.2 (wrzesień 2024)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 20, "text": "Modele są używane głównie w infrastrukturze chmurowej lub przez firmy dysponujące wydajnymi GPU.\n\n## LLaMA 3.2 (wrzesień 2024) Ta aktualizacja rozszerzyła rodzinę o lekkie modele **1 B** i **3 B** (tekstowe) oraz warianty multimodalne **Llama 3.2‑Vision 11 B** i **90 B**. Modele 1 B i 3 B nadal obsługują okna kontekstowe do 128 k tokenów i są trenowane na wielojęzycznym korpusie (angielski, niemiecki, francuski, włoski, portugalski, hindi, hiszpański, tajski)https://huggingface.co/blog/llama32#:~:text =What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 21, "text": "=What%20is%20special%20about%20Llama,2%201B%20and%203B.  \n\n* **Sprzęt** – artykuł Hugging Face podaje, że **model 3 B** wymaga **ok. 6,5 GB VRAM** w bf16/fp16, **3,2 GB** w FP8 i **1,75 GB** w INT4; **model 1 B** potrzebuje **2,5 GB** (FP16), **1,25 GB** (FP8) lub **0,75 GB** (INT4)https://huggingface.co/blog/llama32#:~:text =Llama%203,Language%20Models. Wersja **Vision 11 B** przy 4‑bitowej kwantyzacji używa około **10 GB VRAM**https://huggingface.co/blog/llama32#:~:text =The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 22, "text": "=The%20Vision%20models%20are%20larger%2C,bit%20mode.  \n* **Licencja** – Llama 3.2 jest objęta tą samą licencją co LLaMA 3.1, z dodatkową klauzulą, że osoby lub firmy z siedzibą w UE nie mogą korzystać z wersji multimodalnych (Vision); to ograniczenie nie dotyczy użytkowników końcowych korzystających z aplikacji zintegrowanych z modelamihttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants.  \n* **Zastosowania** – dzięki niewielkim rozmiarom modele 1 B i 3 B są przeznaczone do urządzeń mobilnych i wbudowanych. Modele Vision obsługują tekst i obraz; potrafią rozpoznawać diagramy, tabele i proste elementy wizualne, generując tekstową odpowiedź. ## LLaMA 4 (kwiecień 2025) Czwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 23, "text": "Czwarta generacja wprowadza architekturę **Mixture of Experts (MoE)** oraz natywne przetwarzanie multimodalne. Pierwsze modele to **Llama 4 Scout** i **Llama 4 Maverick**, a w zapowiedzi jest wersja **Llama 4 Behemoth**.  \n\n* **Architektura i kontekst** – w modelach MoE każdy token aktywuje jedynie 17 mld parametrów, mimo że całkowita liczba parametrów jest znacznie większa (Scout ma 109 mld parametrów, 16 ekspertów; Maverick – 400 mld parametrów, 128 ekspertów). Dzięki temu, przy tej samej liczbie aktywnych parametrów, LLaMA 4 uzyskuje lepszą wydajność niż gęste modele."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 24, "text": "Dzięki temu, przy tej samej liczbie aktywnych parametrów, LLaMA 4 uzyskuje lepszą wydajność niż gęste modele. Okna kontekstowe są imponujące: **10 mln tokenów dla Scouta** i **1 mln tokenów dla Mavericka**https://ai.meta.com/blog/llama-4-multimodal-intelligence/#:~:text=,class%20performance%20to%20cost. * ** Wymagania sprzętowe (Scout)**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 25, "text": "Wymagania sprzętowe (Scout)** – analiza portalu Hardware Corner wskazuje, że 4‑bitowa wersja **Llama 4 Scout** wymaga **ok. 55–60 GB VRAM** tylko na wagi (bez pamięci podręcznej) i może być uruchomiona na **pojedynczym GPU NVIDIA H100 80 GB**https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text =These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs. Wersja FP16 wymaga nawet **216 GB** zintegrowanej pamięci na komputerach Apple Mac Studio, a kwantyzacja 3‑bitowa zmniejsza wymagania do ~48 GBhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra.  \n* **Wymagania sprzętowe (Maverick)** – 4‑bitowa wersja modelu 400 B potrzebuje **245 GB VRAM** oraz systemu z **co najmniej 320 GB RAM** lub kilku kart GPU (np. 8×32 GB)https://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 26, "text": "Wersja FP16 wymaga nawet **216 GB** zintegrowanej pamięci na komputerach Apple Mac Studio, a kwantyzacja 3‑bitowa zmniejsza wymagania do ~48 GBhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra.  \n* **Wymagania sprzętowe (Maverick)** – 4‑bitowa wersja modelu 400 B potrzebuje **245 GB VRAM** oraz systemu z **co najmniej 320 GB RAM** lub kilku kart GPU (np. 8×32 GB)https://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text =Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server. Gradient Flow podkreśla, że wariant Maverick wymaga **rozproszonego inferencja** i jest praktycznie nieosiągalny dla użytkowników indywidualnychhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20. * **Licencja**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 27, "text": "* **Licencja** – LLaMA 4 jest udostępniona na tej samej **licencji otwartych wag**, lecz w artykule Gradient Flow wskazuje się, że jest ona nadal restrykcyjna: podmioty o >700 mln użytkowników potrzebują zgody Meta, należy stosować markę „Built with Llama”, przestrzegać polityki akceptowalnego użycia i nie można wykorzystywać modelu do trenowania innych LLM‑ówhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text =No,modification%20but%20includes%20significant%20restrictions. Ponadto z uwagi na przepisy AI Act, wizja w LLaMA 4 może być niedostępna w UEhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=,EU%20due%20to%20regulatory%20concerns.  \n* **Wyniki i zastosowania** – Meta podaje, że Llama 4 Scout przewyższa Mistral 3.1 oraz Gemma 3 w zadaniach multimodalnych i oferuje najlepszy stosunek cena–jakość w swojej klasiehttps://ai.meta.com/blog/llama-4-multimodal-intelligence/#:~:text=,class%20performance%20to%20cost."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 28, "text": "Ponadto z uwagi na przepisy AI Act, wizja w LLaMA 4 może być niedostępna w UEhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=,EU%20due%20to%20regulatory%20concerns.  \n* **Wyniki i zastosowania** – Meta podaje, że Llama 4 Scout przewyższa Mistral 3.1 oraz Gemma 3 w zadaniach multimodalnych i oferuje najlepszy stosunek cena–jakość w swojej klasiehttps://ai.meta.com/blog/llama-4-multimodal-intelligence/#:~:text=,class%20performance%20to%20cost. Llama 4 Maverick ma dorównywać GPT‑4o w benchmarkach reasoningowych, ale wymaga dużej infrastruktury. Modele te są przeznaczone do tworzenia asystentów multimodalnych (tekst + obraz), zaawansowanych aplikacji RAG z ogromnym kontekstem (kilka milionów tokenów) i systemów budujących agentów z długą pamięcią. #"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 29, "text": "# # Podsumowanie tabelaryczne\n\n| Generacja / model | Parametry (aktywnych / całkowitych) | Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 30, "text": "| Maks. okno kontekstowe | Minimalne wymagania pamięci (VRAM/RAM – przy kwantyzacji) | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **LLaMA 1 (7/13/65 B)** | 7 B, 13 B, 65 B gęste | ok. 2 k tokenów | 7 B: ~12–13 GB VRAM (FP16), 3–6 GB VRAM (4‑bit); 13 B: ~24 GB VRAM; 65 B: >130 GB VRAMhttps://www.bacloud.com/en/blog/163/guide-to-gpu-requirements-for-running-ai-models.html#:~:text=LLaMA%20,Meta%20AI | Badawcza (niekomercyjna) | Badania nad fine‑tuningiem, porównania z GPT | \n| **LLaMA 2 (7/13/70 B)** | gęste | ok. 4 k tokenów | 7 B: 6 GB VRAM (Q4), 4 GB RAM (CPU)https://www.hardware-corner.net/llm-database/Llama-2/#:~:text =Below%20are%20the%20Llama,bit%20quantization; 13 B: 10 GB VRAM; 70 B: 40 GB VRAM i 64 GB RAMhttps://www.hardware-corner.net/llm-database/Llama-2/#:~:text=For%2013B%20Parameter%20Models | Llama Community License (komercyjna z limitem 700 mln użytkowników)https://deepsense.ai/blog/llama-2-a-significant-milestone-in-the-world-of-ai/#:~:text =The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 31, "text": "=The%20Llama%202%20license%C2%A0permits%20any,compliant | Generacja i podsumowanie tekstu, asystenci, RAG, fine‑tuning | \n| **Code Llama (7/13/30/34 B)** | gęste | do 100 k tokenów | 7 B: ~6 GB VRAM (Q4); 13 B: 10 GB VRAM; 30/34 B: 20 GB VRAM; 7 B GGML: 4 GB RAMhttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=Hardware%20requirementshttps://www.hardware-corner.net/llm-database/CodeLlama/#:~:text=When%20running%20CodeLlama%20AI%20models%2C,0GB%20of%20RAM | Llama Community License | Generacja kodu, autouzupełnianie, refaktoryzacja | \n| **LLaMA 3 (8 B)** | 8 B gęste | 128 k (GQA) | ~16 GB VRAM (FP16); ~8 GB (FP8); ~4 GB (INT4); plik ~15 GBhttps://parseur.com/blog/blog-llama3-performance-cost#:~:text=,8Bhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB | Llama Community License | Asystenci konwersacyjni, RAG, on‑device inference | \n| **LLaMA 3 (70 B)** | 70 B gęste | 128 k | ~140 GB VRAM (FP16); ~70 GB (FP8); ~35 GB (INT4); uruchomienie wymaga konfiguracji multi‑GPUhttps://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GBhttps://liquidmetal.ai/casesAndBlogs/llama-three-comparison/#:~:text =Pricing | jak wyżej | Analiza i generacja tekstu, RAG w chmurze | \n| **LLaMA 3.1 (8/70/405 B)** | 8 B, 70 B, 405 B gęste | 128 k | 8 B: 16 GB VRAM; 70 B: 140 GB; 405 B: 810 GB (FP16)https://huggingface.co/blog/llama31#:~:text=Model%20Size%20FP16%20FP8%20INT4,405%20GB%20%20203%20GB; dodatkowy narzut ~39 GB na cache dla 128 k tokenówhttps://huggingface.co/blog/llama31#:~:text=significant%20factor,KV%20cache%20memory%20requirements%20are | jak wyżej | Zadania wymagające długiego kontekstu; fine‑tuning w infrastrukturze chmurowej | \n| **LLaMA 3.2 (1/3 B tekstowe)** | 1 B, 3 B gęste | 128 k | 3 B: 6,5 GB VRAM (FP16), 3,2 GB (FP8), 1,75 GB (INT4); 1 B: 2,5 GB (FP16), 1,25 GB (FP8), 0,75 GB (INT4)https://huggingface.co/blog/llama32#:~:text =Llama%203,Language%20Models | jak wyżej (multimodalne ograniczone w UE)https://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Aplikacje mobilne, wbudowane, RAG na urządzeniach brzegowych | \n| **LLaMA 3.2‑Vision (11 B/90 B)** | 11 B, 90 B multimodalne (tekst + obraz) | 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 32, "text": "| 128 k | Wersja 11 B (INT4) wymaga ~10 GB VRAMhttps://huggingface.co/blog/llama32#:~:text=The%20Vision%20models%20are%20larger%2C,bit%20mode; dla 90 B brak publicznych danych, wymaga wielu GPU | jak wyżej, z wyłączeniem UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants | Odpowiadanie na pytania dotyczące obrazów, diagramy, tabele | \n| **LLaMA 4 Scout (17 B aktywnych, 16 ekspertów)** | 17 B aktywnych / 109 B całkowitych (MoE) | 10 mln tokenów | ~55–60 GB VRAM (4‑bit)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text =These%20models%20have%20substantial%20hardware,put%20them%20beyond%20consumer%20GPUs; 61 GB unified memory (4‑bit) na Mac Studiohttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20Unified%20Memory%20Needed%20Recommended,fp16%20216%20GB%20M3%20Ultra; FP16: 216 GB | Licencja open‑weights z ograniczeniami (700 mln MAU, znak „Built with Llama”, polityka AUP)https://gradientflow.com/llama-4-what-you-need-to-know/#:~:text =No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise | \n\n## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 33, "text": "=No,modification%20but%20includes%20significant%20restrictions | Multimodalni asystenci, długie konteksty, RAG z milionami tokenów | \n| **LLaMA 4 Maverick (17 B aktywnych, 128 ekspertów)** | 17 B aktywnych / 400 B całkowitych | 1 mln tokenów | ~245 GB VRAM (4‑bit) i ≥320 GB RAMhttps://www.hardware-corner.net/meta-releases-llama-4-what-hardware/#:~:text=Quantization%20RAM%2FVRAM%20Needed%20Recommended%20Systems,400%20GB%20512GB%20RAM%20server; wymaga rozproszonej inferencjihttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=Llama%204%20Maverick%20 | jak wyżej | Zaawansowane zadania reasoningowe, multimodalne aplikacje klasy Enterprise | \n\n## Wnioski i rekomendacje\n\n1. **Wybór modelu a dostępne zasoby** – mniejsze modele LLaMA (1 B–8 B) można uruchomić na pojedynczej karcie graficznej z 16 GB VRAM lub nawet na CPU z odpowiednią ilością RAM. Modele 70 B i większe wymagają środowisk z wieloma GPU i setkami gigabajtów pamięci. Dla zastosowań RAG w małych firmach rekomendowane są modele 8 B (LLaMA 3) lub 3 B (LLaMA 3.2), które zapewniają dobrą jakość i niskie opóźnienia. 2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 34, "text": "2. **Licencja i obostrzenia** – mimo otwartych wag LLaMA nie jest w pełni open‑source. Użytkownicy muszą przestrzegać licencji, umieszczać oznaczenie „Built with Llama”, nie wykorzystywać modeli do trenowania innych LLM‑ów i uzyskać zgodę przy liczbie użytkowników >700 mlnhttps://gradientflow.com/llama-4-what-you-need-to-know/#:~:text=No,modification%20but%20includes%20significant%20restrictions. Modele multimodalne 3.2 i 4 mają dodatkowe ograniczenia w UEhttps://huggingface.co/blog/llama32#:~:text=Regarding%20the%20licensing%20terms%2C%20Llama,products%20with%20the%20vision%20variants. 3. **Zastosowania RAG**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 35, "text": "3. **Zastosowania RAG** – LLaMA oferuje wydajność i długie konteksty potrzebne do systemów RAG. Modele 3.1 i 4 mogą obsługiwać kontekst od 128 k do milionów tokenów, co pozwala na wbudowanie dużych baz wiedzy bez konieczności intensywnego indeksowania. Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 36, "text": "Jednak ich wymagania sprzętowe i koszty eksploatacji są wysokie – w praktyce lepiej zastosować mniejsze warianty (8 B lub 24 B) i wspierać się mechanizmami przycinania kontekstu. 4. **Trendy rozwojowe** – meta zapowiedziała kolejne warianty LLaMA 4 (m.in. **Behemoth** z 288 B aktywnych parametrów) oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 37, "text": "oraz modele reasoningowe i mniejszych rozmiarów (np. 24 B). Oczekuje się dalszego rozwoju architektur MoE i wysokiej wydajności na układach z połączoną pamięcią (Apple, AMD). Niniejszy plik zestawia stan wiedzy na sierpień 2025 r. i może służyć jako baza wiedzy do systemów RAG. Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\llama.txt", "chunk_id": 38, "text": "Zaleca się śledzenie aktualizacji licencji i nowych wersji, ponieważ Meta często wprowadza nowe modele lub zmienia warunki użycia."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 0, "text": "# Modele Mistral – kompendium (stan na sierpień 2025)\n\n# # Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 1, "text": "Wprowadzenie\n**Mistral AI** to francuski startup założony w 2023 roku, który koncentruje się na budowie nowoczesnych modeli językowych. Mistral udostępnia zarówno modele open‑source (z wagami udostępnionymi w licencji Apache 2.0), jak i tzw. *premier models* dostępne wyłącznie poprzez API. Firma wprowadziła do użycia szereg modeli tekstowych, wielomodalnych, matematycznych, kodowych i głosowych. Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 2, "text": "Poniżej znajduje się szczegółowe zestawienie najważniejszych modeli, ich parametrów, wymagań sprzętowych, licencji, twórców, zastosowań i wybranych benchmarków. ## Podział na rodziny\n- **Modele otwarte** – wagi są dostępne na HuggingFace i można je uruchamiać lokalnie. Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 3, "text": "Podlegają licencji Apache 2.0 (Mistral 7B, Mixtral 8×7B i 8×22B, Codestral Mamba, Mathstral, Mistral NeMo, Pixtral 12B, serie Mistral Small/Devstral Small/Magistral Small, Voxtral Small/Mini itd.). Wymagają znacznej pamięci VRAM podczas inferencji; tabela 1 podaje minimalne wartości.\n- **Modele badawcze (Premier models)** – Mistral Large, Mixtral 8×22B, Pixtral Large, series Magistral Medium i Mistral Medium 3 mają wagi udostępnione tylko na zasadzie licencji badawczej (Mistral Research License) i są przeznaczone do testów niekomercyjnych. Samodzielne użycie wymaga licencji komercyjnej."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 4, "text": "Samodzielne użycie wymaga licencji komercyjnej. - ** Modele komercyjne/API only** – Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 5, "text": "– Mistral Medium 3, Magistral Medium, Devstral Medium, Voxtral Mini Transcribe, Mistral Embed, Codestral Embed i Mistral OCR to usługi dostępne w platformie La Plateforme (API) oraz u partnerów chmurowych. Użytkownik wchodzi z nimi w relację usługową. # #"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 6, "text": "# Tabela 1 – skrócone parametry i wymagania VRAM Poniższa tabela przedstawia najważniejsze otwarte modele Mistral wraz z liczbą parametrów, długością kontekstu, minimalnym wymogiem pamięci GPU (VRAM) do inferencji zgodnie z oficjalną tabelą rozmiarówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208, typem licencji i przykładowymi zastosowaniami. Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 7, "text": "Wymagania VRAM dotyczą pełnej precyzji (bf16/fp16); wersje skwantyzowane mogą wymagać mniej pamięci (np. 4‑bitowy Mistral 7B zmieści się w ≈4 GB VRAMhttps://www.hardware-corner.net/llm-database/Mistral/#:~:text=If%20the%207B%20%20model,to%20run%20that%20one%20smoothly). | Model | Parametry (B) / aktywne | Kontekst | Min. VRAM (GB)https://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,Mini%203B%203B%208 | Licencja | Przykładowe zastosowania |\n|---|---|---|---|---|---|\n| **Mistral 7B** | 7.3 B | 8k tokens | 16 GB | Apache 2.0 | lokalne asystenty, RAG, podstawowe generowanie tekstu |\n| **Mixtral 8×7B** | 46.7 B (12.9 B aktywne) | 32k tokens | 100 GB | Apache 2.0 | wydajna mieszanka ekspertów (MoE) do długich odpowiedzi, RAG |\n| **Mixtral 8×22B** | 140.6 B (39.1 B aktywne) | 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 8, "text": "| 64k tokens | 300 GB | Apache 2.0 | mocniejsza MoE do złożonych zadań, długich kontekstów |\n| **Codestral 22B** | 22.2 B | 32k tokens | 60 GB | Mistral AI Non‑Production License (MNPL) | generacja kodu, fill‑in‑the‑middle, testy jednostkowe |\n| **Codestral Mamba 7B** | 7.3 B | (potencjalnie nieskończony kontekst) | 16 GB | Apache 2.0 | szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 9, "text": "| szybkie generowanie kodu dzięki architekturze Mamba |\n| **Mathstral 7B** | 7.3 B | 32k tokens | 16 GB | Apache 2.0 | rozwiązywanie zadań matematycznych, MATH/MMLU |\n| **Mistral NeMo 12B** | 12 B | do 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text =Mistral,fp8 | Apache 2.0 | wielojęzyczny model tekstowy z Tekken tokenizerem; zamiennik Mistral 7B |\n| **Pixtral 12B** | 12 B + 0.4 B vision encoder | 128k tokens | 28 GB (bf16) / 16 GB (fp8)https://docs.mistral.ai/getting-started/models/weights/#:~:text =Pixtral,fp8 | Apache 2.0 | multimodalny model (obrazy + tekst); obsługa RAG i wizji |\n| **Mistral Small 24B** (wersje 3/3.1/3.2) | 24 B | 32k–128k tokens | 60 GB | Apache 2.0 | szybkie modele do lokalnych asystentów, z obsługą funkcji i wizji |\n| **Magistral Small 24B** | 24 B | 40k tokens | 60 GB | Apache 2.0 | model rozumowania, transparencja, łańcuchy myśli |\n| **Devstral Small 24B** | 24 B | 128k tokens | 60 GB | Apache 2.0 | agent kodeksowy, automatyczne poprawki kodu |\n| **Voxtral Small 24B** | 24 B | 32k tokens | 60 GB | Apache 2.0 | model audio (transkrypcja i zrozumienie mowy) |\n| **Voxtral Mini 3B** | 3 B | 32k tokens | 8 GB | Apache 2.0 | kompaktowy model audio do edge‑device |\n| **Ministral 8B** | 8 B | 128k tokens (32k w vLLM) | 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 10, "text": "| 24 GB | Mistral Commercial/Research License | niskolatencyjne asystenty on‑device |\n| **Ministral 3B** | 3 B | 128k tokens (32k vLLM) | 8 GB | Mistral Commercial/Research License | edge computing, offline asystenty |\n| **Mistral Large 2 (24.07/24.11)** | 123 B | 128k tokens | 250 GB | Mistral Research License | duże modele do złożonych zadań, RAG, wysoka jakość |\n| **Pixtral Large** | 124 B + 1 B vision encoder | 128k tokens | 250 GB | Mistral Research License | frontowy model multimodalny (wizja + tekst) |\n\n## Opisy poszczególnych modeli\n### Mistral 7B\nPierwszy model Mistral AI z września 2023 r.; 7.3 mld parametrów, kontekst 8k. Dostępny w wersji podstawowej i „instruct”."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 11, "text": "Dostępny w wersji podstawowej i „instruct”. Publikacja w licencji Apache 2.0 umożliwia lokalne uruchomienie. Artykuły sprzętowe zalecają co najmniej 12‑rdzeniowy procesor oraz kartę GPU z ≥12 GB VRAM; dzięki kwantyzacji 4‑bitowej model można uruchomić nawet na karcie z 4 GB VRAMhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Here%E2%80%99s%20a%20quick%20checklist%20to,if%20your%20system%20is%20readyhttps://www.oneclickitsolution.com/centerofexcellence/aiml/run-mistral-7b-locally-hardware-software-specs#:~:text=Depending%20on%20which%20version%20of,fair%20amount%20of%20storage%20space. Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 12, "text": "Zmiana na 16‑bit (bf16) wymaga ok. 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2046.7B%2012.9B%20100. Model obsługuje generację tekstu, podstawowe programowanie i RAG; jest punktem wyjściowym dla wielu nowszych modeli. ### Mixtral 8×7B Model typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 13, "text": "Model typu *mixture‑of‑experts* (MoE) składający się z 8 ekspertów po 7 mld parametrów; aktywnych jest ok. 12.9 mld parametrówhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060, dzięki czemu zapewnia wysoką jakość przy mniejszym koszcie obliczeniowym. Model oferuje kontekst 32k tokenów, licencję Apache 2.0 oraz możliwość uruchamiania lokalnego. Dyskusje na HuggingFace podają, że wersja 4‑bitowa wymaga ~22. 5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 14, "text": "5 GB VRAM, 8‑bitowa ~45 GB, a pełna półprecyzja ~90 GBhttps://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/discussions/3#:~:text=Quick%20math%20,parameters. Artykuł AIME sugeruje minimalną konfigurację 4× RTX A6000 48 GB dla trybu bf16https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text=Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model sprawdza się w długim RAG, generowaniu kodu i zadań wymagających dłuższego kontekstu. ### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 15, "text": "### Mixtral 8×22B\nKolejny model MoE złożony z ośmiu ekspertów po 22 mld parametrów (aktywnych 39.1 mld)https://docs.mistral.ai/getting-started/models/weights/#:~:text =Name%20Number%20of%20parameters%20Number,v0.1%2022. 2B%2022.2B%2060. Oferuje 64k tokenów kontekstu i licencję Apache 2.0. Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 16, "text": "Wersja bf16 wymaga około 263 GB VRAM, natomiast kwantyzacja int4 redukuje wymóg do około 65 GBhttps://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1/discussions/2#:~:text=Model%20Memory%20Requirements. Artykuł AIME podaje, że w praktyce potrzebne są 4× GPU 80 GB (A100/H100)https://www.aime.info/blog/en/how-to-run-and-deploy-mixtral/#:~:text =Hardware%20Requirements%20of%20Mixtral%208x7B,and%208x22B. Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 17, "text": "Model przeznaczony jest do złożonych zadań (analiza dokumentów, generowanie długich sekwencji), ale od marca 2025 r. jest wycofywany na rzecz nowszych modeli. ## # Codestral 22B (seria 25.01/25. 08)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 18, "text": "08) Codestral to open‑weight model przeznaczony do generowania kodu, wytrenowany na 80+ językach programowania z kontekstem 32k. Oferuje tryb *fill‑in‑the‑middle* (FIM) do uzupełniania brakujących fragmentów kodu i osiąga wysokie wyniki w HumanEval i MBPP. Wersja 22 mld parametrów wymaga ok. 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 19, "text": "Wersja 22 mld parametrów wymaga ok. 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Name%20Number%20of%20parameters%20Number,v0.1%2022.2B%2022.2B%2060. Licencja MNPL pozwala na badania i projekty niekomercyjnehttps://docs.mistral.ai/getting-started/models/weights/#:~:text=,are%20under%20Mistral%20Research%20License; do zastosowań komercyjnych konieczne jest wykupienie licencji. W lipcu 2025 r. wprowadzono wersję 25.08, która zwiększyła odsetek akceptowanych uzupełnień kodu o 30 % i zmniejszyła liczbę „runaway generations” o 50 %https://mistral.ai/news/codestral-25-08#:~:text=Today%2C%20we%20announce%20its%20latest,measurable%20upgrades%20over%20prior%20versions. Codestral jest dostępny w stacku Mistral Code oraz w usłudze Codestral Embed (embedding do wyszukiwania kodu)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 20, "text": "Codestral jest dostępny w stacku Mistral Code oraz w usłudze Codestral Embed (embedding do wyszukiwania kodu). ### Codestral Mamba 7B Pierwszy model Mistral w architekturze Mamba2, specjalizowany w generowaniu kodu. Ma 7.285 mld parametrów i korzysta z liniowej złożoności obliczeniowej, co teoretycznie pozwala na nieskończony kontekst."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 21, "text": "Ma 7.285 mld parametrów i korzysta z liniowej złożoności obliczeniowej, co teoretycznie pozwala na nieskończony kontekst. Udostępniony w licencji Apache 2.0https://mistral.ai/news/codestral-mamba#:~:text=As%20a%20tribute%20to%20Cleopatra%2C,0%20license, sprawdza się jako lokalny asystent programistyczny o niskiej latencji. Minimalne wymagania VRAM są porównywalne z Mistral 7B (około 16 GB). ##"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 22, "text": "## # Mathstral 7B Model o 7 mld parametrach z kontekstem 32k przeznaczony do zadań matematycznych i STEM."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 23, "text": "Model o 7 mld parametrach z kontekstem 32k przeznaczony do zadań matematycznych i STEM. Współpraca z projektem Numina zaowocowała wysokimi wynikami: 56,6 % na zbiorze MATH i 63,47 % na MMLUhttps://mistral.ai/news/mathstral#:~:text=As%20a%20tribute%20to%20Archimedes%2C,0%20license. Model, dostępny na licencji Apache 2.0, zachowuje zgodność z architekturą Mistral 7B, dzięki czemu może pełnić rolę matematycznego eksperta w RAG. ##"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 24, "text": "## # Mistral NeMo 12B Model 12 mld parametrów wytrenowany we współpracy z NVIDIA. Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 25, "text": "Oferuje aż 128k kontekstu, jest wielojęzyczny i obsługuje wywoływanie funkcji. Zastosowano nowy tokenizer Tekken, który kompresuje kod źródłowy i wiele języków lepiej niż SentencePiecehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%3A%20our%20new%20best,0%20licensehttps://mistral.ai/news/mistral-nemo#:~:text=Mistral%20NeMo%20uses%20a%20new,of%20all%20languages. Wersja bf16 wymaga ok. 28 GB VRAM, natomiast dzięki uczeniu z kwantyzacją model można uruchamiać w fp8 na GPU z 16 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,fp8. NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.\n\n###"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 26, "text": "NeMo jest publikowany w licencji Apache 2.0 i jest następcą Mistral 7B.\n\n### Mistral Large 2 i Mistral Large 24.11\nMistral Large 2, ogłoszony w lipcu 2024 r., to flagowy model Mistrala z 123 mld parametrów i kontekstem 128k. Blog „Large Enough” podkreśla, że model znacząco poprawia generowanie kodu, matematyczne rozumowanie i obsługę wielu językówhttps://mistral.ai/news/mistral-medium-3#:~:text=Medium%20is%20the%20new%20large. Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GBhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2407/discussions/15#:~:text=You%20will%20need%20about%20,to%20train%20it%20using%20Adam."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 27, "text": "Model wydany jest w licencji badawczej; w pełnej precyzji wymaga ok. 233 GB VRAM, a kwantyzacja int4 redukuje wymagania do ~58 GBhttps://huggingface.co/mistralai/Mistral-Large-Instruct-2407/discussions/15#:~:text=You%20will%20need%20about%20,to%20train%20it%20using%20Adam. Wersja 24.11 (Mistral Large 2.1) z listopada 2024 r. wprowadziła nowe podpowiedzi systemowe, lepsze wywoływanie funkcji i ulepszone wnioskowanie długokontekstowehttps://mistral.ai/news/pixtral-large#:~:text=Along%20with%20Pixtral%20Large%2C%20Mistral,Mistral%20AI%20for%20commercial%20use.\n\n### Pixtral 12B i Pixtral Large\nPixtral 12B to pierwszy multimodalny model Mistrala. Składa się z 12 mld parametrów w dekoderze (na bazie Mistral NeMo) i 400 mln parametrów w enkoderze obrazu; model przyjmuje kilka obrazów oraz tekst, obsługuje zmienne rozmiary obrazów i ma 128k kontekstuhttps://mistral.ai/news/pixtral-12b#:~:text=Pixtral%2012B%20in%20short%3Ahttps://mistral.ai/news/pixtral-12b#:~:text=Pixtral%20was%20trained%20to%20be,instruction%20following%2C%20coding%2C%20and%20math. Jest dostępny w licencji Apache 2.0; w precyzji bf16 wymaga 28 GB VRAM, w fp8 około 16 GBhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8;"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 28, "text": "Jest dostępny w licencji Apache 2.0; w precyzji bf16 wymaga 28 GB VRAM, w fp8 około 16 GBhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Pixtral,fp8; artykuł Ori podaje, że do uruchomienia na vLLM wystarczy 24 GB VRAMhttps://www.ori.co/blog/how-to-run-pixtral-on-a-cloud-gpu#:~:text=memory%20for%20this%20demo%20because,Debian%20is%20also%20an%20option. Pixtral Large (124 mld parametrów + 1 mld w enkoderze obrazu) ma kontekst 128k i przewyższa konkurencję na benchmarkach MathVista, DocVQA i VQA v2https://mistral.ai/news/pixtral-large#:~:text=Pixtral%20Large%20in%20short%3A. Udostępniony jest w licencji badawczej; wymaga ok. 250 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,124B%20124B%20250.\n\n### Mistral Small 3, 3.1 i 3.2 (24 B parametrów)"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 29, "text": "Udostępniony jest w licencji badawczej; wymaga ok. 250 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,124B%20124B%20250.\n\n### Mistral Small 3, 3.1 i 3.2 (24 B parametrów) Seria Mistral Small to 24‑miliardowe modele zoptymalizowane pod kątem niskiej latencji. Wersja 3 z stycznia 2025 r. oferuje 81 % accuracy na MMLU i przepływ 150 tokenów/s; model został zaprojektowany tak, aby zmieścić się na pojedynczej karcie RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3#:~:text=Mistral%20Small%203%20is%20a,performance%2C%20with%20very%20low%20latencyhttps://mistral.ai/news/mistral-small-3#:~:text=,a%20Macbook%20with%2032GB%20RAM. Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3-1#:~:text=performance%2C%20multimodal%20understanding%2C%20and%20an,of%20150%20tokens%20per%20second."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 30, "text": "Wersja 3.1 (marzec 2025) dodaje możliwość obsługi obrazów, funkcji i kontekstu 128k; nadal można ją uruchamiać na RTX 4090 lub MacBooku z 32 GB RAMhttps://mistral.ai/news/mistral-small-3-1#:~:text=performance%2C%20multimodal%20understanding%2C%20and%20an,of%20150%20tokens%20per%20second. Wersja 3.2 (czerwiec 2025) to aktualizacja skupiająca się na lepszym przestrzeganiu instrukcji, redukcji powtórzeń i solidniejszym wywoływaniu funkcji – wewnętrzne miary wzrosły z 82,75 % do 84,78 % na testach instruction‑followinghttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks; zewnętrzne testy Wildbench i Arena Hard znacznie się poprawiłyhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=The%20improvements%20in%20Mistral%20Small,are%20evident%20across%20various%20benchmarks. 3.2 wymaga ~55 GB VRAM w bf16, a dzięki kwantyzacji może działać na kartach z 32 GB RAMhttps://blog.shinkai.com/mistral-ai-unveils-mistral-small-3-2-enhanced-capabilities-for-ai-integration/#:~:text=Mistral%20Small%203,serve%20access%20and%20direct%20deployment. Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 31, "text": "Modele z serii Small są dostępne w licencji Apache 2.0 i stanowią fundament do fine‑tuningów, asystentów lokalnych, RAG oraz zadań multimodalnych. ## # Magistral Small (24 B) i Magistral Medium Magistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 32, "text": "Magistral to rodzina modeli *reasoning* zapowiedziana w czerwcu 2025 r. Blog Mistral AI opisuje ją jako dwa warianty: Magistral Small (24 B) udostępniony na licencji Apache 2.0 oraz Magistral Medium (wersja korporacyjna)https://mistral.ai/news/magistral#:~:text=Today%2C%20we%E2%80%99re%20excited%20to%20announce,along%20with%20deep%20multilingual%20flexibility. Modele zostały zaprojektowane z myślą o transparentnym łańcuchu rozumowania (chain‑of‑thought), wielojęzyczności i obsłudze domen specjalistycznych. Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 33, "text": "Wersja Medium uzyskała 73,6 % na konkursie AIME2024 (90 % w trybie majority‑vote), natomiast Small osiągnął 70,7 %https://mistral.ai/news/magistral#:~:text=,a%20more%20powerful%2C%20enterprise%20version. Magistral Small ma kontekst 40k i minimalny wymóg 60 GB VRAMhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Mistral,2507%2024B%2024B%2060; może być pobrany z HuggingFace i samodzielnie wdrożonyhttps://mistral.ai/news/magistral#:~:text=Availability. Magistral Medium jest dostępny poprzez API i dla klientów chmurowych; Mistral podkreśla szybkość (Flash Answers w Le Chat) oraz zastosowania w prawie, finansach i regulacjachhttps://mistral.ai/news/magistral#:~:text=With%20Flash%20Answers%20in%20Le,and%20user%20feedback%2C%20at%20scale. ##"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 34, "text": "## # Devstral Small 1.1 i Devstral Medium Devstral to rodzina modeli agentowych do pisania kodu. W lipcu 2025 r. udostępniono Devstral Small 1.1 pod licencją Apache 2.0 oraz Devstral Medium jako usługę APIhttps://mistral.ai/news/devstral-2507#:~:text=Today%2C%20we%20introduce%20Devstral%20Medium%2C,different%20prompts%20and%20agentic%20scaffolds."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 35, "text": "W lipcu 2025 r. udostępniono Devstral Small 1.1 pod licencją Apache 2.0 oraz Devstral Medium jako usługę APIhttps://mistral.ai/news/devstral-2507#:~:text=Today%2C%20we%20introduce%20Devstral%20Medium%2C,different%20prompts%20and%20agentic%20scaffolds. Devstral Small 1.1 zachował architekturę 24 B, lecz osiągnął 53,6 % w benchmarku SWE‑Bench Verifiedhttps://mistral.ai/news/devstral-2507#:~:text=Enhanced%20Performance, oferując lepszą generalizację i obsługę formatu XMLhttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Small%201,of%20applications%20and%20agentic%20scaffolds. Devstral Medium uzyskał 61,6 % na SWE‑Bench i jest dostępny do wdrożeń on‑premises; model można dostroić pod konkretne repozytoriahttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Medium%20builds%20upon%20the,effective%20model. W ekosystemie Mistral Code"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 36, "text": "W ekosystemie Mistral Code Devstral współpracuje z embederami (Codestral Embed) i agentami (OpenHands). Devstral Small zmieści się na pojedynczym RTX 4090 lub Macu z 32 GB RAMhttps://mistral.ai/news/codestral-25-08#:~:text=,open%20models%20by%20wide%20margins. ### Voxtral Small 24 B i Voxtral Mini 3 B\nVoxtral to pierwsza rodzina otwartych modeli audio firmy Mistral."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 37, "text": "### Voxtral Small 24 B i Voxtral Mini 3 B\nVoxtral to pierwsza rodzina otwartych modeli audio firmy Mistral. Ogłoszona w lipcu 2025 r., składa się z wersji Small (24 B) i Mini (3 B). Oba modele są udostępnione na licencji Apache 2.0, a Voxtral Mini Transcribe (3 B) jest zoptymalizowany tylko do transkrypcjihttps://mistral.ai/news/voxtral#:~:text=We%20release%20the%20Voxtral%20models,behind%20Voxtral%2C%20please%20refer%20to. Modele oferują kontekst 32k tokenów, co pozwala przetwarzać 30‑minutowe nagrania do transkrypcji lub 40‑minutowe nagrania w trybie rozumieniahttps://mistral.ai/news/voxtral#:~:text=include%3A."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 38, "text": "Modele oferują kontekst 32k tokenów, co pozwala przetwarzać 30‑minutowe nagrania do transkrypcji lub 40‑minutowe nagrania w trybie rozumieniahttps://mistral.ai/news/voxtral#:~:text=include%3A. Voxtral obsługuje wielojęzyczność, wbudowane Q&A i podsumowania audio, wywoływanie funkcji bezpośrednio z głosu oraz zachowuje możliwości tekstowe swojego trzonu (Mistral Small 3.1)https://mistral.ai/news/voxtral#:~:text=%2A%20Long,or%2040%20minutes%20for%20understanding. Modele są gotowe do pobrania na HuggingFace; wymagają ok. 60 GB VRAM (Small) lub 8 GB (Mini)https://docs.mistral.ai/getting-started/models/weights/#:~:text =Devstral,Mini%203B%203B%208.  Bench­marki publikowane w artykule pokazują, że Voxtral znacząco przewyższa Whisper large‑v3 i GPT‑4o mini w dokładności transkrypcji i rozumieniahttps://mistral.ai/news/voxtral#:~:text=Voxtral%20comprehensively%20outperforms%20Whisper%20large,demonstrating%20its%20strong%20multilingual%20capabilities.\n\n### Mistral Saba\nMistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 39, "text": "=Devstral,Mini%203B%203B%208.  Bench­marki publikowane w artykule pokazują, że Voxtral znacząco przewyższa Whisper large‑v3 i GPT‑4o mini w dokładności transkrypcji i rozumieniahttps://mistral.ai/news/voxtral#:~:text=Voxtral%20comprehensively%20outperforms%20Whisper%20large,demonstrating%20its%20strong%20multilingual%20capabilities.\n\n### Mistral Saba\nMistral Saba to 24‑miliardowy model językowy zaprojektowany na potrzeby Bliskiego Wschodu i Azji Południowej. Artykuł z lutego 2025 r. opisuje go jako model szkolony na starannie dobranych danych arabskich i językach indyjskichhttps://mistral.ai/news/mistral-saba#:~:text=Mistral%20Saba%20is%20a%2024B,serve%20as%20a%20strong%20base. Saba obsługuje języki takie jak arabski i tamil, zapewniając bardziej naturalne odpowiedzi niż modele kilkakrotnie większehttps://mistral.ai/news/mistral-saba#:~:text=from%20across%20the%20Middle%20East,over%20150%20tokens%20per%20second. Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowiskuhttps://mistral.ai/news/mistral-saba#:~:text=to%20train%20highly%20specific%20regional,over%20150%20tokens%20per%20second."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 40, "text": "Może działać lokalnie na pojedynczym GPU, oferując ponad 150 tokenów/s, i jest dostępny poprzez API oraz do samodzielnego wdrożenia w bezpiecznym środowiskuhttps://mistral.ai/news/mistral-saba#:~:text=to%20train%20highly%20specific%20regional,over%20150%20tokens%20per%20second. Zastosowania obejmują wirtualną obsługę klienta po arabsku, eksperckie doradztwo w branżach (energia, finanse, zdrowie) i tworzenie treści kulturowychhttps://mistral.ai/news/mistral-saba#:~:text=As%20Mistral%20Saba%20gains%20traction,emerging%2C%20showcasing%20the%20model%27s%20versatility. Wymagania sprzętowe są zbliżone do serii Small (ok. 60 GB VRAM). ###"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 41, "text": "### Mistral Medium 3\nModel Mistral Medium 3 (maj 2025) jest pierwszym „średnim” modelem w ofercie Mistral AI. Artykuł „Medium is the new large” podkreśla, że model zapewnia wydajność zbliżoną do modeli premium, przy ośmiokrotnie niższym koszcie i upro­sz­czonym wdrożeniuhttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20state,with%20radically%20simplified%20enterprise%20deployments. Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPUhttps://mistral.ai/news/mistral-medium-3#:~:text=Additionally%2C%20Mistral%20Medium%203%20can,of%20four%20GPUs%20and%20above."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 42, "text": "Medium 3 jest dostępny w formie API; można go także wdrożyć on‑premises na zestawie czterech GPUhttps://mistral.ai/news/mistral-medium-3#:~:text=Additionally%2C%20Mistral%20Medium%203%20can,of%20four%20GPUs%20and%20above. Model jest przeznaczony do zastosowań korporacyjnych: programowania, multimodalnej analizy i zadaniowych agentów; benchmarki pokazują, że osiąga ponad 90 % wyników Claude Sonnet 3.7 przy niższej ceniehttps://mistral.ai/news/mistral-medium-3#:~:text=Mistral%20Medium%203%20delivers%20frontier,2%20output%20per%20M%20token. Mistral Medium nie posiada publicznej tabeli wymagań VRAM; w praktyce wymaga wielu GPU.\n\n### Magistral Medium 1.1\nW lipcu 2025 r. firma ogłosiła aktualizację magistral Medium 1.1. Według changelogu, nowa wersja z datą 25.07 ma kontekst 40k i jest dostępna poprzez APIhttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Premier%20models."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 43, "text": "Według changelogu, nowa wersja z datą 25.07 ma kontekst 40k i jest dostępna poprzez APIhttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Premier%20models. Ponieważ model jest premierowy, wagi nie są publiczne; licencja to Mistral Research/Commercial. ## # Devstral Medium"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 44, "text": "# Devstral Medium Jak opisano w poście „Upgrading agentic coding capabilities with the new Devstral models”, Devstral Medium osiąga 61,6 % na SWE‑Bench Verified i jest dostępny poprzez API. Model umożliwia wdrożenia on‑premises oraz dostrojenie pod specyficzne repozytoria koduhttps://mistral.ai/news/devstral-2507#:~:text=Devstral%20Medium%20builds%20upon%20the,effective%20model. Podobnie jak Mistral Medium, jest to model premierowy, a zatem wagi i wymagania VRAM nie są publiczne.\n\n### Codestral Embed\nCodestral Embed to model embeddingowy wyspecjalizowany do kodu."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 45, "text": "Podobnie jak Mistral Medium, jest to model premierowy, a zatem wagi i wymagania VRAM nie są publiczne.\n\n### Codestral Embed\nCodestral Embed to model embeddingowy wyspecjalizowany do kodu. Wersja 25.05 umożliwia generowanie wektorów o zmiennym wymiarze (np. 256 wymiarów w int8), przewyższając modele Voyage Code 3, Cohere Embed v4. 0 i OpenAI Embeddingshttps://mistral.ai/news/codestral-embed#:~:text=We%20are%20excited%20to%20release,world%20code%20data. Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie koduhttps://mistral.ai/news/codestral-embed#:~:text=Use%20cases."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 46, "text": "Embed jest dostępny w API; oferuje elastyczną kontrolę kosztów, szybkie wyszukiwanie w repozytoriach, semantyczne wyszukiwanie, wykrywanie duplikatów i klastrowanie koduhttps://mistral.ai/news/codestral-embed#:~:text=Use%20cases. Zalecana długość kontekstu to 8 k tokenów, a do RAG zaleca się dzielenie dokumentów na fragmenty ~3000 znaków z nawarstwieniem 1000 znakówhttps://mistral.ai/news/codestral-embed#:~:text=For%20retrieval%20use%20cases%2C%20while,for%20more%20information%20about%20chunking. ## # Mistral Embed"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 47, "text": "# Mistral Embed Mistral Embed to model embeddingowy dla tekstu (8 k kontekstu) dostępny jako usługa API. Został zaprojektowany do generowania gęstych reprezentacji semantycznych dla RAG i wyszukiwania. Chociaż Mistral nie opublikował dedykowanego bloga, dokumentacja API opisuje model jako dostarczający wektory 1024‑elementowe; usługa jest wykorzystywana w platformach takich jak Pinecone, Zilliz czy LangChain."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 48, "text": "Chociaż Mistral nie opublikował dedykowanego bloga, dokumentacja API opisuje model jako dostarczający wektory 1024‑elementowe; usługa jest wykorzystywana w platformach takich jak Pinecone, Zilliz czy LangChain. ### Mistral OCR Mistral OCR to usługa rozpoznawania tekstu z dokumentów (PDF, obrazy). Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 49, "text": "Model potrafi ekstraktować tekst wraz z obrazami z złożonych dokumentów, tabel, równań i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Mistral%20OCR%20is%20an%20Optical,ordered%20interleaved%20text%20and%20images. Benchmarki pokazują, że przewyższa Google Document AI, Gemini, Azure OCR i GPT‑4o w rozpoznawaniu matematyki, tabel i wielu językówhttps://mistral.ai/news/mistral-ocr#:~:text=Top. Jest bardzo szybki – przetwarza do 2000 stron na minutę na pojedynczym węźlehttps://mistral.ai/news/mistral-ocr#:~:text=Fastest%20in%20its%20category. Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 50, "text": "Mistral OCR jest dostępny jako API w wersji 2503/2505; brak jest otwartych wag. ### Ministral 3B i 8B\nSeria Ministral to kompaktowe modele do zastosowań on‑device/edge. Zostały ogłoszone w październiku 2024 r. (3B i 8B) i udostępnione w licencji Mistral Commercial/Research. Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 51, "text": "Modele obsługują do 128k kontekstu (32k w vLLM) i wykorzystują technikę *interleaved sliding‑window attention* (dla wersji 8B) w celu zmniejszenia zapotrzebowania na pamięć i czas. Używane są jako prywatne asystenty, do tłumaczenia offline, analityki na urządzeniach i robotykihttps://mistral.ai/news/ministraux#:~:text=Introducing%20the%20world%E2%80%99s%20best%20edge,models. Minimalne wymagania VRAM to 8 GB dla 3B i 24 GB dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 52, "text": "dla 8Bhttps://docs.mistral.ai/getting-started/models/weights/#:~:text=Voxtral,3B%203B%208. Wersje bazowe są dostępne na HuggingFace do użytku badawczego; licencja komercyjna wymagana jest do zastosowań produkcyjnychhttps://mistral.ai/news/ministraux#:~:text=Ministral%208B%20ministral,Mistral%20Commercial%20License. # #"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 53, "text": "# Modele wycofane Zgodnie z dokumentacją, starsze modele Mistral (Mistral 7B, Mixtral 8×7B, Mixtral 8×22B oraz wersje Large 2407) zostały oznaczone jako przestarzałe i zostaną wycofane z platformy 30 marca 2025 r. lub 16 czerwca 2025 r. dla innych wersjihttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Legacy%20modelshttps://docs.mistral.ai/getting-started/models/models_overview/#:~:text=Mistral%20Medium%202312%20%60mistral,latest%20%60%20Mistral%20Large%202407%E2%9C%94%EF%B8%8F. Użytkownicy powinni migrować do nowszych modeli."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 54, "text": "Użytkownicy powinni migrować do nowszych modeli. # # Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 55, "text": "Podsumowanie\nEkosystem Mistral AI rozwija się bardzo dynamicznie: od kompaktowego Mistral 7B, przez hybrydowe modele Mixtral, wyspecjalizowane Mathstral i Codestral, po multimodalne Pixtral i Voxtral, modele reasoning (Magistral), agentowe (Devstral), embeddingowe oraz usługi OCR. Wiele modeli jest open‑source (Apache 2.0), co ułatwia ich wykorzystanie w projektach RAG i lokalnych asystentach, a ich wymogi pamięci zaczynają się od 8 GB VRAM (Voxtral Mini, Ministral 3B). Modele premierowe (Medium 3, Magistral Medium, Devstral Medium, Mistral Large, Pixtral Large) dostępne są poprzez API i adresują potrzeby korporacyjne związane z niską latencją, dużym kontekstem i wielomodalnością. Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\mistal.txt", "chunk_id": 56, "text": "Dzięki licencjom badawczym i komercyjnym Mistral AI umożliwia zarówno eksperymentowanie, jak i produkcyjne zastosowania, oferując równocześnie narzędzia do własnego dostrajania (mistral‑inference, mistral‑finetune) oraz infrastruktury (Mistral Compute)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 0, "text": "# PLLuM – polski model językowy\n\n## Tło i motywacja\n\nPLLuM (Polish Large Language Model) jest projektem konsorcjum polskich uczelni i instytutów, koordynowanym przez Politechnikę Wrocławską i wspieranym m.in. przez NASK PIB, Instytut Podstaw Informatyki PAN, Ośrodek Przetwarzania Informacji PIB, Uniwersytet Łódzki oraz Instytut Slawistyki PANhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Model został zaprezentowany 24 lutego 2025 roku przez Ministerstwo Cyfryzacji i ma służyć administracji publicznej, biznesowi oraz naucehttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversity. Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. \n\n## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 1, "text": "Twórcy podkreślają, że PLLuM powstał w oparciu o etycznie pozyskane dane, a wersje przeznaczone do użytku komercyjnego korzystają z treści licencjonowanych przez właścicieli, natomiast modele naukowe („nc”) opierają się także na publicznie dostępnych zbiorach jak Common Crawlhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. \n\n## Zbiory danych i fine‑tuning\n\n- **Korpus treningowy** – do budowy PLLuM zebrano wysokiej jakości dane tekstowe w języku polskim (~150 miliardów tokenów) oraz teksty w językach słowiańskich, bałtyckich i angielskim. Ze względów prawnych modele open‑source są trenowane na ok. 28 miliardach tokenów dostępnych komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations.  \n- **Instrukcje „organiczne”** – stworzono największy polski zestaw manualnie przygotowanych instrukcji (~40 000 par zapytanie‑odpowiedź), obejmujący około 3,5 tys. dialogów wieloetapowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,training%20phase. - **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 2, "text": "- **Corpus preferencji** – opracowano pierwszy polski korpus preferencji, w którym zespół annotatorów ocenił różne odpowiedzi modeli, co uczy model równowagi i bezpieczeństwahttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,potentially%20controversial%20or%20adversarial%20topics. - **Fine‑tuning i alignacja** – modele były dalej dostrajane na danych instrukcyjnych (40 k oryginalnych instrukcji, ~50 k instrukcji przetłumaczonych z korpusów premium oraz ok. 10 k syntetycznych) oraz na korpusie preferencjihttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. Dodatkowo opracowano wyspecjalizowane modele RAG dla administracji publicznejhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering. #"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 3, "text": "# # Dostępne modele PLLuM\n\nTabela poniżej podsumowuje najważniejsze warianty PLLuM wraz z liczbą parametrów, kontekstem, bazą i licencją oraz przybliżonym zapotrzebowaniem na pamięć (VRAM/RAM) przy wnioskowaniu. Wymagania VRAM przyjęto na podstawie rozmiarów plików GGUF (które z grubsza odpowiadają pamięci potrzebnej do załadowania modelu) oraz znanych wymagań modeli bazowych (Llama 3.1, Mistral Nemo, Mixtral 8×7B). W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 4, "text": "W przypadku pełnej precyzji (bf16/fp16) wymagane jest znacznie więcej pamięci – te liczby podano w komentarzach. | Model | Parametry i baza | Kontekst i zastosowania | Licencja | Szacowane wymagania pamięci podczas wnioskowania* |\n|---|---|---|---|---|\n| **Llama‑PLLuM‑8B (base/instruct/chat)** | 8 mld parametrów, oparte na **Llama 3.1‑8B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text =Model%20Name%20Params%20License%20Based,8B."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 5, "text": "=Model%20Name%20Params%20License%20Based,8B. | Kontekst do 128 tys. tokenów (wynik dziedziczony z Llama 3.1). Przeznaczone do ogólnych zadań językowych w języku polskim – generacja, streszczanie, Q&A, chat – oraz jako fundament dla aplikacji domenowychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Intended%20Use%20Cases. | Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 6, "text": "| Llama 3.1 Community License (modele bez sufiksu „nc” mogą być używane komercyjnie). Modele „nc” mają licencję CC‑BY‑NC‑4. 0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text =The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 7, "text": "=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. | Przybliżone rozmiary kwantyzacji: Q4\\_K\\_S ok. **4,7 GB**, Q4\\_K\\_M ok. **4,9 GB**, Q5\\_K\\_M ok. **5,6 GB**, pełne FP16 ~**16,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-8B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,2%2016%20bpw%2C%20overkill. Te wielkości odzwierciedlają minimalne VRAM wymagane do uruchomienia modelu; w pełnej precyzji 16 bitów potrzeba ok. 16 GB VRAM. |\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 8, "text": "|\n| **PLLuM‑12B (base/instruct/chat)** | 12 mld parametrów; bazą jest **Mistral Nemo 12B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text =PLLuM,2407, model z kontekstem 128 k tokenów oraz udoskonaloną tokenizacją Tekken. | Używane do zaawansowanych zadań językowych, z możliwością funkcji w RAG; 12B wersje „nc” zawierają dane z szerszego korpusu (150 mld tokenów). | Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 9, "text": "| Apache 2.0 dla modeli open; wersje z sufiksem „nc” – licencja CC‑BY‑NC‑4. 0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text =PLLuM,2407. | Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 10, "text": "| Wersja Mistral Nemo 12B wymaga ok. **12 GB VRAM** przy kontekście 16 k (wg dyskusji HF)https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/22#:~:text =At%20least%2012%20GB%20of,are%20many%20exl2%20out%20there. Kwantyzacje GGUF: Q2\\_K **4,9 GB**, Q4\\_K\\_M **7,6 GB**, Q5\\_K\\_M **8,8 GB**, Q6\\_K **10,2 GB**, Q8\\_0 **13,1 GB**https://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text =Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 11, "text": "=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. |\n| **PLLuM‑70B (base/instruct/chat)** | 70 mld parametrów; oparte na **Llama 3.1‑70B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Llama,70B. | Służą do złożonych zadań w języku polskim (długie konwersacje, zaawansowana analiza tekstu). | Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 12, "text": "| Llama 3.1 Community License (licencja nie‑komercyjna w wersjach „nc”). | Modele 70B wymagają dużo pamięci: pełna precyzja fp16 ok. **140 GB VRAM**, int4 ~ **35 GB** (wg wytycznych Llama 3.1). Kwantyzacje GGUF: Q4\\_K\\_S **40,4 GB**, Q4\\_K\\_M **42,6 GB**, Q5\\_K\\_M **50,0 GB**, Q6\\_K **58,0 GB**, Q8\\_0 **75,1 GB**https://huggingface.co/mradermacher/Llama-PLLuM-70B-instruct-GGUF#:~:text=,IQ%20quants. |\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 13, "text": "|\n| **PLLuM‑8×7B (base/instruct/chat)** | Mixture‑of‑Experts 8×7B (około 45 mld aktywnych parametrów); bazą jest **Mixtral 8×7B**https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=PLLuM. Model MoE działa tak, że w czasie wnioskowania używanych jest tylko 2–4 ekspertów, co zmniejsza zużycie pamięci. | Długi kontekst (max. 128 k), wysoka jakość wnioskowania; dobre do złożonych zadań administracyjnych i RAG. | Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 14, "text": "| Apache 2.0 dla modeli open; wersje „nc” – licencja CC‑BY‑NC‑4. 0https://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text =PLLuM. | Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 15, "text": "| Orientacyjne wymagania VRAM na podstawie Mixtral 8×7B: 4‑bitowa kwantyzacja (~**22,5 GB** VRAM), 8‑bit ~**45 GB**, pełna fp16 ~**90 GB** (z kontekstem ok. 32 k). Modele „nc” mogą korzystać z rozszerzonego korpusu. |\n\n\\* *Wymagania VRAM zależą od długości kontekstu i ustawień, ale w tabeli podano minimalne wielkości pamięci potrzebnej do załadowania wag w danej precyzji. *\n\n## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa)."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 16, "text": "*\n\n## Zastosowania\n\nPLLuM jest zaprojektowany jako podstawowa warstwa dla wielu zastosowań w języku polskim:\n\n- **Generacja i analiza tekstu** – podstawowe zadania generacji, streszczania, tłumaczenia i analizy sentymentu; modele 8B i 12B zapewniają szybkie odpowiedzi, a 70B oferuje najwyższą jakość i kontekst do długich dokumentów.\n- **Asystenci dla administracji publicznej** – specjalne modele RAG potrafią odpowiadać na pytania dotyczące procedur administracyjnych czy ustaw, korzystając z dedykowanych baz dokumentówhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=%2A%20Domain,information%20retrieval%20and%20question%20answering.\n- **Systemy pytanie‑odpowiedź (QA) i chat** – wersje „chat” i „instruct” są dostrojone do interakcji z użytkownikiem i generują bezpieczne, adekwatne odpowiedzi.\n- **Projekty badawcze i fine‑tuning** – zbiory instrukcji i preferencji umożliwiają dalsze dostosowanie modeli do specyficznych domen (np. medycyny, prawa). Programiści mogą korzystać z licencji open‑source (Apache 2.0 lub Llama 3.1) albo z licencji CC‑BY‑NC‑4. 0 w przypadku modeli naukowych.\n\n# #"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 17, "text": "# Zalety i wyróżniki\n\n1. **Bogate zbiory danych** – korpus liczący ~150 mld tokenów oraz unikatowe zbiory instrukcji i preferencji czynią PLLuM najbardziej zaawansowanym polskim modelem językowymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulations. 2. ** Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 18, "text": "Skalowalna rodzina modeli** – użytkownicy mogą wybrać wariant od 8 mld do 70 mld parametrów, a także model Mixture‑of‑Experts 8×7B, dobierając balans między wydajnością a zapotrzebowaniem sprzętowymhttps://primotly.com/article/pllum-polish-large-language-model-artificial-intelligence#:~:text=PLLuM%20was%20officially%20presented%20on,linguistic%20accuracy%20and%20thematic%20diversityhttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,on%C2%A0the%20Mixture%20of%20Experts%20architecture. 3. **Etyczne pozyskanie danych i bezpieczna alignacja** – twórcy stosują licencjonowane treści i uwzględniają polskie przepisy o prawach autorskich, a także manualnie anotują preferencje, co ogranicza ryzyko halucynacjihttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20modelshttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,based%20%28Retrieval%20Augmented. 4. **Dostępność**"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 19, "text": "4. **Dostępność** – modele są dostępne na platformie Hugging Face i można je integrować z biblioteką Transformers; wersje open są objęte licencjami pozwalającymi na komercyjne wykorzystanie, a wersje „nc” dedykowane są badaniom i zastosowaniom niekomercyjnymhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and. ## Ograniczenia\n\n- **Wysokie wymagania sprzętowe** – duże modele (70B lub 8×7B) wymagają znacznej pamięci VRAM (do kilkudziesięciu GB, a przy pełnej precyzji fp16 nawet powyżej 90 GB); mniejsze warianty 8B i 12B są bardziej przyjazne dla GPU klasy konsumenckiejhttps://huggingface.co/mradermacher/PLLuM-12B-base-250801-GGUF#:~:text=Link%20Type%20Size%2FGB%20Notes%20GGUF,1%20fast%2C%20best%20quality. - **"}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 20, "text": "- ** Ograniczenia licencyjne ** – modele oparte na Llama 3.1 podlegają licencji społecznościowej Meta, która wymaga uwzględnienia klauzul dotyczących użycia, m.in. konieczności podawania informacji „Built with Llama 3” w produktach i zakazu wykorzystywania do trenowania konkurencyjnych modeli. Modele „nc” mają licencję CC‑BY‑NC‑4."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 21, "text": "Modele „nc” mają licencję CC‑BY‑NC‑4. 0 i nie mogą być używane komercyjniehttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=The%20models%20with%20fully%20open,aligned%20on%20human%20preferences%20and.  \n- **Potencjalne halucynacje i uprzedzenia** – mimo że opracowano korpus preferencji i liczne testy, modele mogą generować niepoprawne treści lub wykazywać stronniczość w kwestiach wrażliwychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=Limitations%20and%20Bias.\n\n## Podsumowanie\n\nPLLuM to wszechstronna rodzina polskich modeli językowych, obejmująca warianty 8B, 12B, 70B oraz 8×7B, zbudowane na otwartych modelach Llama 3.1, Mistral Nemo oraz Mixtral. Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models."}
{"source": "C:\\Users\\mikol\\PycharmProjects\\chatPA\\docs\\pllum.txt", "chunk_id": 22, "text": "Projekt wyróżnia się ogromnym korpusem treningowym, etycznym pozyskiwaniem danych i dużą liczbą instrukcji oraz danych preferencji, co przekłada się na wysoką jakość generowanych odpowiedzi i możliwość adaptacji do zadań publicznych i komercyjnychhttps://huggingface.co/CYFRAGOVPL/PLLuM-12B-instruct#:~:text=,compliance%20with%20relevant%20legal%20regulationshttps://opi.org.pl/en/pllum-model-wspoltworzony-przez-opi-pib-gotowy-do-dzialania/#:~:text=%E2%80%A2%20The%20Polish%20language%20model,RAG%29%20models. Dzięki dostępności w różnych rozmiarach i licencjach PLLuM może być używany w badaniach, biznesie i administracji, wspierając rozwój ekosystemu #AIMadeInPoland."}
